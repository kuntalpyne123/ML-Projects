# -*- coding: utf-8 -*-
"""agent_graph_(final).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvfVOwycNBhFhowKVNDok_h-lug8aVyw
"""

# Install required packages with proper version constraints
!pip install -U -q "google-generativeai>=0.8.0" langgraph langchain-core streamlit "pandas==2.2.2" scikit-learn matplotlib "langchain-google-genai>=2.0.0"

!pip install --upgrade google-generativeai

# --- 0. Import Dependencies and Setup ---
import os
import google.generativeai as genai
from typing import List, Dict, Any, Literal
from typing_extensions import TypedDict, Annotated
import operator
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END, START
from google.colab import userdata # For Colab secrets

print(f"Google GenAI version: {genai.__version__}")

# --- 1. Define the Agent State ---
class AgentState(TypedDict):
    """
    The state of our Agentic C-Suite.

    This TypedDict defines the structure of the data that is passed
    between nodes.
    """

    # The user's initial high-level request
    initial_request: str

    # The structured plan from the Project Manager
    project_plan: str

    # The findings and citations from the Market Researcher
    research_findings: Dict[str, Any]

    # The analysis, code output, and plot paths from the Financial Analyst
    analysis_results: Dict[str, Any]

    # The final, synthesized report from the Business Analyst
    final_report: str

    # A list to hold the full conversation history.
    messages: Annotated[List[AnyMessage], operator.add]

# --- 2. Helper Functions ---
def get_gemini_client(model_name: str = "gemini-2.5-flash"):
    """
    Initialize and return the Gemini GenerativeModel.
    """
    key = userdata.get('GEMINI_API_KEY')
    if not key:
        raise ValueError("GEMINI_API_KEY secret not found in Colab. Please add it using the 'Key' icon on the left.")

    # Set the API key for the client
    genai.configure(api_key=key)

    try:
        model = genai.GenerativeModel(model_name)
        print(f"Gemini client initialized successfully with model: {model_name}")
        return model
    except Exception as e:
        print(f"Error initializing model: {e}")
        return None

def _to_genai_content(messages: List[AnyMessage]) -> List[Dict]:
    """
    Convert LangChain messages to Google GenAI format.
    FIXED: Removes system messages since Gemini API doesn't support them.
    """
    contents = []
    for message in messages:
        # Skip SystemMessage as Gemini API doesn't support system role
        if isinstance(message, SystemMessage):
            continue

        elif isinstance(message, HumanMessage):
            contents.append({
                "role": "user",
                "parts": [{"text": message.content}]
            })
        elif isinstance(message, AIMessage):
            contents.append({
                "role": "model",
                "parts": [{"text": message.content}]
            })
    return contents

def create_prompt_with_context(system_prompt: str, user_prompt: str) -> str:
    """
    Combine system prompt and user prompt into a single message.
    This bypasses the system role limitation.
    """
    return f"{system_prompt}\n\nUser Request: {user_prompt}"

# --- 3. Initialize the Client ---
client = get_gemini_client()

if not client:
    raise RuntimeError("Failed to initialize Gemini client. Please check your API key.")

print("Setup completed successfully!")

# --- 4. Define the Agent Node Functions ---

def run_pm_node(state: AgentState) -> dict:
    """
    Node 1: The Project Manager.
    Deconstructs the user's request into a step-by-step plan.
    """
    print("--- Running Project Manager Node ---")
    system_prompt = (
        "You are an expert Project Manager for an elite AI financial analysis team. "
        "Your team consists of a Market Researcher, a Financial Analyst, and a Business Analyst. "
        "A new user request has come in. Your *only* job is to analyze this request and "
        "create a clear, structured, and actionable step-by-step plan for your team. "
        "The plan must be a bulleted list. "
        "The plan should explicitly state: "
        "1. What the Market Researcher needs to investigate. "
        "2. What the Financial Analyst needs to calculate. "
        "3. What the Business Analyst needs to deliver in the final report."
    )

    # Get the last message (the user's request)
    user_request_message = state["messages"][-1]

    # FIXED: Combine system prompt and user request into a single prompt
    combined_prompt = create_prompt_with_context(system_prompt, user_request_message.content)

    try:
        # Call the model with the combined prompt
        response = client.generate_content(combined_prompt)

        # Update the state
        return {
            "project_plan": response.text,
            "messages": [AIMessage(content=f"**Project Manager's Plan:**\n{response.text}")]
        }
    except Exception as e:
        print(f"Error in PM node: {e}")
        return {
            "project_plan": f"Error: {e}",
            "messages": [AIMessage(content=f"**Project Manager's Plan:**\nError occurred: {e}")]
        }

def run_market_researcher_node(state: AgentState) -> dict:
    """
    Node 2: The Market Researcher.
    Executes research using native Google Search grounding.
    """
    print("--- Running Market Researcher Node ---")
    system_prompt = (
        "You are the Market Researcher for the team. "
        "You have been given a project plan. Your task is to execute the "
        "market research portion of this plan *only*. "
        "Use your built-in Google Search tool to find the most recent, relevant, "
        "and high-quality information. Synthesize your findings into a concise "
        "summary and provide citations for all claims."
    )

    prompt = f"""
    Here is the project plan:
    {state['project_plan']}

    Here is the original user request:
    {state['initial_request']}

    Please execute the market research phase now.
    """

    combined_prompt = create_prompt_with_context(system_prompt, prompt)

    # --- Enable "Google Search Tool" ---
    # Note: "Google Search Tool" requires specific model support
    try:
        # Try to create Google Search Tool
        from google.generativeai.types import file_types
        #google_search_tool = file_types.GoogleSearch()
        response = client.generate_content(
            combined_prompt,
            tools=[genai.GenerativeModel.Tool.GOOGLE_SEARCH]
        )
        print("✓ Google Search Tool enabled successfully")
    except Exception as e:
        print(f"Google Search tool not available, falling back to regular generation: {e}")
        response = client.generate_content(combined_prompt)

    # Parse the response and citations
    summary = response.text
    citations = []

    # Check for grounding metadata (citations)
    if hasattr(response, 'grounding_metadata') and response.grounding_metadata:
        try:
            for chunk in response.grounding_metadata.grounding_chunks:
                citations.append({
                    "title": chunk.title,
                    "uri": chunk.uri
                })
        except Exception as e:
            print(f"Error parsing citations: {e}")

    findings = {
        "summary": summary,
        "citations": citations
    }

    citations_text = "\n".join([f"- [{c['title']}]({c['uri']})" for c in citations])
    message_content = f"**Market Researcher's Findings:**\n{summary}\n\n**Sources:**\n{citations_text}"

    return {
        "research_findings": findings,
        "messages": [AIMessage(content=message_content)]
    }

def run_financial_analyst_node(state: AgentState) -> dict:
    """
    Node 3: The Financial Analyst.
    Performs quantitative analysis using native Code Execution.
    """
    print("--- Running Financial Analyst Node ---")
    system_prompt = (
        "You are the senior Financial Analyst. You have a project plan "
        "and market research findings. Your task is to perform deep quantitative analysis. "
        "You have a secure Python code execution environment with 'pandas', 'numpy', "
        "'scikit-learn', and 'matplotlib' pre-installed. "
        "You *must* write and execute Python code to answer the analytical questions. "
        "When you generate plots, save them to disk (e.g., `plt.savefig('plot1.png')`). "
        "After your code, provide a final text summary along with vizualizations of your key findings."
    )

    prompt = f"""
    Here is the project plan:
    {state['project_plan']}

    Here are the market research findings:
    {state['research_findings']['summary']}

    Please execute the financial analysis phase. Write and run code to
    generate quantitative insights and visualizations.
    """

    combined_prompt = create_prompt_with_context(system_prompt, prompt)

    # --- Enable "Code execution tool" ---
    try:
        # Try to create Code execution tool
        from google.generativeai.types import file_types
        code_execution_tool = file_types.code_execution_tool()
        response = client.generate_content(
            combined_prompt,
            tools=[code_execution_tool]
        )
        print("✓ Code execution tool enabled successfully")
    except Exception as e:
        print(f"Code execution tool not available, falling back to regular generation: {e}")
        response = client.generate_content(combined_prompt)

    # Parse the complex multi-part code response
    code_outputs = []
    text_summary = ""
    if not response.candidates:
      raise ValueError("No valid response returned from Financial Analyst model.")

    # Parse response parts
    try:
        for part in response.candidates[0].content.parts:
            if part.text:
                text_summary += part.text
            elif hasattr(part, 'executable_code') and part.executable_code:
                code_outputs.append(f"--- CODE EXECUTED ---\n{part.executable_code.code}\n")
            elif hasattr(part, 'code_execution_result') and part.code_execution_result:
                code_outputs.append(f"--- CODE OUTPUT (stdout/stderr) ---\n{part.code_execution_result.output}\n")
    except Exception as e:
        print(f"Error parsing response parts: {e}")
        text_summary = response.text

    analysis_package = {
        "summary": text_summary,
        "stdout": "\n".join(code_outputs),
    }

    message_content = f"**Financial Analyst's Summary:**\n{text_summary}\n\n"
    if code_outputs:
        message_content += f"**Code Workbench Output:**\n```python\n{''.join(code_outputs)}\n```"

    return {
        "analysis_results": analysis_package,
        "messages": [AIMessage(content=message_content)]
    }

def run_business_analyst_node(state: AgentState) -> dict:
    """
    Node 4: The Business Analyst.
    Synthesizes all information into a final, C-suite-ready report.
    """
    print("--- Running Business Analyst Node ---")
    system_prompt = (
        "You are the Chief Business Analyst. Your team has completed its work. "
        "Your *only* job is to synthesize all their outputs (plan, research, analysis) "
        "into a single, comprehensive, and polished C-suite-level final report. "
        "This is the final document for the user. Weave all information into a "
        "single, coherent narrative."
        "include any important vizualizations in your report as well. "
    )

    # Create a comprehensive prompt with ALL data from the state
    full_context = f"""
    Here is all the information gathered by your team:

    ORIGINAL USER REQUEST:
    {state['initial_request']}

    PROJECT MANAGER'S PLAN:
    {state['project_plan']}

    MARKET RESEARCHER'S FINDINGS:
    {state['research_findings']}

    FINANCIAL ANALYST'S REPORT:
    {state['analysis_results']}

    Please now synthesize all of this information into a single, final report.
    """

    combined_prompt = create_prompt_with_context(system_prompt, full_context)

    # Call generate_content
    response = client.generate_content(combined_prompt)

    return {
        "final_report": response.text,
        "messages": [AIMessage(content=response.text)]
    }

# --- 5. Define the Graph Structure ---
workflow = StateGraph(AgentState)

# Add the nodes
workflow.add_node("project_manager", run_pm_node)
workflow.add_node("market_researcher", run_market_researcher_node)
workflow.add_node("financial_analyst", run_financial_analyst_node)
workflow.add_node("business_analyst", run_business_analyst_node)

# Add the edges to enforce the strict sequential workflow
workflow.set_entry_point("project_manager")
workflow.add_edge("project_manager", "market_researcher")
workflow.add_edge("market_researcher", "financial_analyst")
workflow.add_edge("financial_analyst", "business_analyst")
workflow.add_edge("business_analyst", END)

# --- 6. Compile the Graph ---
app = workflow.compile()

print("--- Graph Compiled Successfully! ---")
print("The 'app' variable is now ready to be used.")

# --- 7. Test the Setup (FIXED VERSION) ---
import google.generativeai as genai
from google.colab import userdata

print(f"Using google-generativeai version: {genai.__version__}")

# Test 1: Simple Text Generation (FIXED)
try:
    print("\n--- TEST 1: Simple Text Generation ---")
    key = userdata.get('GEMINI_API_KEY')
    if not key:
        print("ERROR: GEMINI_API_KEY not found in Colab secrets")
    else:
        genai.configure(api_key=key)
        model = genai.GenerativeModel("gemini-2.5-flash")

        prompt = "Explain the concept of 'market capitalization' in one sentence."
        response = model.generate_content(prompt)
        print(f"Model Response:\n{response.text}")
        print("✓ Test 1 PASSED")
except Exception as e:
    print(f"✗ Test 1 FAILED: {e}")

# Test 2: Test Message Conversion (FIXED)
try:
    print("\n--- TEST 2: Message Conversion (System role removed) ---")
    messages = [SystemMessage(content="You are a helpful assistant."), HumanMessage(content="Hello!")]
    contents = _to_genai_content(messages)
    print(f"Converted {len(messages)} messages to {len(contents)} content blocks (System messages skipped)")

    # Verify no system roles
    for content in contents:
        assert content['role'] != 'system', "Should not have system roles"
    print("✓ Test 2 PASSED - No system roles in output")
except Exception as e:
    print(f"✗ Test 2 FAILED: {e}")

# Test 3: Test Combined Prompt Function
try:
    print("\n--- TEST 3: Combined Prompt Function ---")
    system = "You are a helpful assistant."
    user = "Hello, how are you?"
    combined = create_prompt_with_context(system, user)

    assert "You are a helpful assistant" in combined
    assert "Hello, how are you" in combined
    print(f"✓ Test 3 PASSED - Combined prompt contains both system and user content")
except Exception as e:
    print(f"✗ Test 3 FAILED: {e}")

print("\n--- Setup Tests Complete ---")

# --- 8. Example Usage (FIXED VERSION) ---
def run_financial_analysis(user_request: str):
    """
    Run a complete financial analysis workflow.
    FIXED: Now works with the corrected API calls.
    """
    print(f"\n{'='*50}")
    print(f"Starting Financial Analysis for: {user_request}")
    print(f"{'='*50}")

    # Initialize the state with the user's request
    initial_state = {
        "initial_request": user_request,
        "project_plan": "",
        "research_findings": {},
        "analysis_results": {},
        "final_report": "",
        "messages": [HumanMessage(content=user_request)]
    }

    try:
        # Run the workflow
        final_state = app.invoke(initial_state)

        print("\n" + "="*50)
        print("FINAL REPORT")
        print("="*50)
        print(final_state["final_report"])

        return final_state

    except Exception as e:
        print(f"Error running analysis: {e}")
        # Return partial state if available
        return None

# Example usage (Uncomment to test):
result = run_financial_analysis("I want to know everything about the recent Jane Street scandal.")

# Alternative simpler test:
# result = run_financial_analysis("What are the key trends in the electric vehicle market?")